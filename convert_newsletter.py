"""
NewsletterAI - ì§€ìì²´ ì†Œì‹ì§€ ë³€í™˜ ìŠ¤í¬ë¦½íŠ¸
ê´‘ëª…ì†Œì‹ 649í˜¸ PDF -> ëª¨ë°”ì¼ ìµœì í™” HTML

ğŸ¤– Powered by NewsletterAI
"""

import fitz  # PyMuPDF
import base64
import os
import sys
import json
from datetime import datetime
from pathlib import Path

# í”„ë¡œì íŠ¸ ê²½ë¡œ ì¶”ê°€
sys.path.insert(0, str(Path(__file__).parent))

from vision_ocr import VisionOCR
from newsletter_html_generator import NewsletterHTMLGenerator

# í’ˆì§ˆ ê²€ì‚¬ ëª¨ë“ˆ (ìˆìœ¼ë©´ ì‚¬ìš©)
try:
    from learning_data.newsletter.ocr_validator import OCRValidator, NewsletterQualityChecker
    QUALITY_CHECK_AVAILABLE = True
except ImportError:
    QUALITY_CHECK_AVAILABLE = False


def extract_page_images(pdf_path: str, output_dir: Path, dpi: int = 150) -> dict:
    """
    PDFì—ì„œ í˜ì´ì§€ë³„ ì´ë¯¸ì§€ë¥¼ ì¶”ì¶œí•˜ì—¬ ì €ì¥

    Args:
        pdf_path: PDF íŒŒì¼ ê²½ë¡œ
        output_dir: ì´ë¯¸ì§€ ì €ì¥ ë””ë ‰í† ë¦¬
        dpi: ì´ë¯¸ì§€ í•´ìƒë„ (ê¸°ë³¸ 150, ì›ë¬¸ë³´ê¸°ìš©)

    Returns:
        {page_num: image_path} ë”•ì…”ë„ˆë¦¬
    """
    images_dir = output_dir / "pages"
    images_dir.mkdir(parents=True, exist_ok=True)

    page_images = {}
    doc = fitz.open(pdf_path)

    for page_num in range(len(doc)):
        page = doc[page_num]

        # í˜ì´ì§€ë¥¼ ì´ë¯¸ì§€ë¡œ ë³€í™˜
        mat = fitz.Matrix(dpi/72, dpi/72)
        pix = page.get_pixmap(matrix=mat)

        # ì´ë¯¸ì§€ ì €ì¥
        image_path = images_dir / f"page_{page_num + 1}.jpg"
        pix.save(str(image_path), "jpeg")

        # ìƒëŒ€ ê²½ë¡œ ì €ì¥ (HTMLì—ì„œ ì‚¬ìš©)
        page_images[page_num + 1] = f"pages/page_{page_num + 1}.jpg"

        print(f"  [{page_num + 1}p] ì´ë¯¸ì§€ ì €ì¥: {image_path.name}")

    doc.close()
    return page_images


def convert_newsletter_pdf(pdf_path: str, output_path: str = None, city_name: str = "ê´‘ëª…ì‹œ", extract_images: bool = True):
    """
    ì§€ìì²´ ì†Œì‹ì§€ PDFë¥¼ HTMLë¡œ ë³€í™˜

    Args:
        pdf_path: PDF íŒŒì¼ ê²½ë¡œ
        output_path: HTML ì¶œë ¥ ê²½ë¡œ (Noneì´ë©´ ìë™ ìƒì„±)
        city_name: ì§€ìì²´ ì´ë¦„
        extract_images: í˜ì´ì§€ ì´ë¯¸ì§€ ì¶”ì¶œ ì—¬ë¶€ (ì›ë¬¸ë³´ê¸° ê¸°ëŠ¥ìš©)
    """
    print(f"\n{'='*60}")
    print(f"[NewsletterAI] ì§€ìì²´ ì†Œì‹ì§€ PDF ë³€í™˜")
    print(f"{'='*60}")
    print(f"ì…ë ¥: {pdf_path}")
    print(f"ì§€ìì²´: {city_name}")

    # Vision OCR ì´ˆê¸°í™”
    vision_ocr = VisionOCR()
    if not vision_ocr.client:
        print("ì˜¤ë¥˜: ANTHROPIC_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤!")
        return None

    # PDF ì—´ê¸°
    doc = fitz.open(pdf_path)
    page_count = len(doc)
    print(f"PDF í˜ì´ì§€ ìˆ˜: {page_count}")

    # ëª¨ë“  í˜ì´ì§€ ë¶„ì„
    all_pages_data = []

    for page_num in range(page_count):
        print(f"\n[{page_num + 1}/{page_count}] í˜ì´ì§€ ë¶„ì„ ì¤‘...")

        page = doc[page_num]

        # í˜ì´ì§€ë¥¼ ì´ë¯¸ì§€ë¡œ ë³€í™˜ (DPI 250 - OCR ì •í™•ë„ì™€ API ì œí•œ ê· í˜•)
        mat = fitz.Matrix(250/72, 250/72)
        pix = page.get_pixmap(matrix=mat)
        img_data = pix.tobytes("jpeg")

        # Base64 ì¸ì½”ë”©
        image_base64 = base64.b64encode(img_data).decode('utf-8')

        # Claude Visionìœ¼ë¡œ ë¶„ì„
        result = vision_ocr.extract_newsletter_info(
            image_base64=image_base64,
            media_type="image/jpeg",
            page_number=page_num + 1,
            total_pages=page_count
        )

        structured = result.get("structured", {})
        print(f"  - ì¹´í…Œê³ ë¦¬: {structured.get('category', 'N/A')}")
        print(f"  - ê¸°ì‚¬ ìˆ˜: {len(structured.get('articles', []))}")
        print(f"  - í˜„ì¥ì·¨ì¬: {len(structured.get('field_reports', []))}")
        print(f"  - ì¸í„°ë·°: {len(structured.get('interviews', []))}")

        all_pages_data.append({
            "page_num": page_num + 1,
            "text": result.get("text", ""),
            "structured": structured
        })

    doc.close()

    # í’ˆì§ˆ ê²€ì‚¬ ë° ìë™ êµì •
    if QUALITY_CHECK_AVAILABLE:
        print(f"\n{'='*60}")
        print("[í’ˆì§ˆê²€ì‚¬] OCR ê²°ê³¼ ê²€ì¦ ì¤‘...")

        quality_checker = NewsletterQualityChecker()
        quality_report = quality_checker.check_newsletter_quality(all_pages_data)

        print(f"  - ì „ì²´ í’ˆì§ˆ ì ìˆ˜: {quality_report['overall_score']}/100")
        print(f"  - ê¶Œì¥ì‚¬í•­: {quality_report['recommendation']}")

        if quality_report['pages_needing_reprocess']:
            print(f"  - ì¬ì²˜ë¦¬ í•„ìš” í˜ì´ì§€: {quality_report['pages_needing_reprocess']}")

        # ìë™ êµì • ì ìš©
        validator = OCRValidator()
        for page_data in all_pages_data:
            structured = page_data.get("structured", {})

            # ê¸°ì‚¬ í…ìŠ¤íŠ¸ êµì •
            for article in structured.get("articles", []):
                if article.get("summary"):
                    corrected, _ = validator.auto_correct(article["summary"])
                    article["summary"] = corrected
                if article.get("content"):
                    corrected, _ = validator.auto_correct(article["content"])
                    article["content"] = corrected

            # í˜„ì¥ì·¨ì¬ í…ìŠ¤íŠ¸ êµì •
            for report in structured.get("field_reports", []):
                if report.get("content"):
                    corrected, _ = validator.auto_correct(report["content"])
                    report["content"] = corrected

            # ì¸í„°ë·° í…ìŠ¤íŠ¸ êµì •
            for interview in structured.get("interviews", []):
                if interview.get("quote"):
                    corrected, _ = validator.auto_correct(interview["quote"])
                    interview["quote"] = corrected

        print("  - ìë™ êµì • ì™„ë£Œ")

    # ë°ì´í„° í†µí•©
    print(f"\n{'='*60}")
    print("ë°ì´í„° í†µí•© ë° HTML ìƒì„± ì¤‘...")

    merged_data = _merge_newsletter_data(all_pages_data, city_name)

    # HTML ìƒì„±
    generator = NewsletterHTMLGenerator(city_name)
    html_content = generator.generate(merged_data)

    # ì¶œë ¥ ê²½ë¡œ ì„¤ì •
    if not output_path:
        output_dir = Path("outputs/Newsletter") / city_name
        output_dir.mkdir(parents=True, exist_ok=True)
        timestamp = datetime.now().strftime("%Y%m%d_%H%M")
        output_path = output_dir / f"newsletter_{timestamp}.html"
    else:
        output_dir = Path(output_path).parent
        output_dir.mkdir(parents=True, exist_ok=True)

    # í˜ì´ì§€ ì´ë¯¸ì§€ ì¶”ì¶œ (ì›ë¬¸ë³´ê¸° ê¸°ëŠ¥ìš©)
    page_images = {}
    if extract_images:
        print(f"\n[ì´ë¯¸ì§€ ì¶”ì¶œ] í˜ì´ì§€ ì´ë¯¸ì§€ ì¶”ì¶œ ì¤‘...")
        page_images = extract_page_images(pdf_path, output_dir)

        # HTMLì— ì´ë¯¸ì§€ ê²½ë¡œ ì£¼ì…
        page_images_js = json.dumps(page_images)
        html_content = html_content.replace(
            "const PAGE_IMAGES = {}; // í˜ì´ì§€ ì´ë¯¸ì§€ ê²½ë¡œ (convert_newsletter.pyì—ì„œ ì„¤ì •)",
            f"const PAGE_IMAGES = {page_images_js}; // ì›ë¬¸ ì´ë¯¸ì§€ ê²½ë¡œ"
        )

    # HTML ì €ì¥
    with open(output_path, "w", encoding="utf-8") as f:
        f.write(html_content)

    print(f"\n[ì™„ë£Œ] ë³€í™˜ ì™„ë£Œ!")
    print(f"HTML: {output_path}")
    if page_images:
        print(f"ì´ë¯¸ì§€: {len(page_images)}ê°œ í˜ì´ì§€")
    print(f"{'='*60}\n")

    return str(output_path)


def _merge_newsletter_data(pages_data: list, city_name: str) -> dict:
    """í˜ì´ì§€ ë°ì´í„° í†µí•©"""
    merged = {
        "title": "ê´‘ëª…ì†Œì‹",
        "issue": "",
        "date": "",
        "publisher": "",
        "city": city_name,
        "contact": {
            "phone": "02-2680-2062",
            "email": "gmgongbo@korea.kr",
            "website": "www.gm.go.kr"
        },
        "pages": []
    }

    # ì²« í˜ì´ì§€ì—ì„œ ê¸°ë³¸ ì •ë³´ ì¶”ì¶œ
    if pages_data:
        first_page = pages_data[0].get("structured", {})
        merged["issue"] = first_page.get("issue", "ì œ649í˜¸")
        merged["date"] = first_page.get("date", "2025ë…„ 10ì›” 29ì¼")
        merged["publisher"] = first_page.get("publisher", "ê´‘ëª…ì‹œì¥ ë°•ìŠ¹ì›")

    # ì¹´í…Œê³ ë¦¬ë³„ë¡œ í˜ì´ì§€ ê·¸ë£¹í™”
    category_pages = {}

    for page in pages_data:
        structured = page.get("structured", {})
        page_num = structured.get("page_num", page.get("page_num", 0))

        # ì¹´í…Œê³ ë¦¬ ê²°ì •
        category = structured.get("category", "")
        if not category:
            if page_num == 1:
                category = "íŠ¹ì§‘"
            elif page_num in [2, 3, 4]:
                category = "ìŠ¤ë§ˆíŠ¸ ë„ì‹œ"
            elif page_num in [5]:
                category = "ë³µì§€"
            elif page_num in [6, 7]:
                category = "ê³µë™ì²´"
            elif page_num in [8, 9]:
                category = "ëŒë´„"
            elif page_num in [10, 11]:
                category = "ìƒí™œì •ë³´"
            elif page_num == 12:
                category = "ìºë¦­í„°"
            else:
                category = "ê¸°íƒ€"

        if category not in category_pages:
            category_pages[category] = []

        page_data = {
            "page_num": page_num,
            "category": category,
            "main_title": structured.get("main_title", ""),
            "page_title": structured.get("page_title", ""),  # í˜ì´ì§€ ì œëª©
            "page_desc": structured.get("page_desc", ""),    # í˜ì´ì§€ ì„¤ëª…
            "subtitle": structured.get("subtitle", ""),
            "articles": structured.get("articles", []),
            "field_reports": structured.get("field_reports", []),
            "interviews": structured.get("interviews", []),
            "info_items": structured.get("info_items", []),
            "characters": structured.get("characters", [])   # ìºë¦­í„° ì •ë³´
        }

        # ê¸°ì‚¬ì— ì¹´í…Œê³ ë¦¬ ì¶”ê°€
        for article in page_data["articles"]:
            if not article.get("category"):
                article["category"] = category

        category_pages[category].append(page_data)

    # í˜ì´ì§€ ëª©ë¡ ìƒì„±
    for category, pages in category_pages.items():
        for page in pages:
            merged["pages"].append(page)

    return merged


if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description="NewsletterAI - ì§€ìì²´ ì†Œì‹ì§€ PDF ë³€í™˜")
    parser.add_argument("--pdf", type=str, help="PDF íŒŒì¼ ê²½ë¡œ")
    parser.add_argument("--city", type=str, default="ê´‘ëª…ì‹œ", help="ì§€ìì²´ ì´ë¦„ (ê¸°ë³¸ê°’: ê´‘ëª…ì‹œ)")
    parser.add_argument("--output", type=str, help="ì¶œë ¥ HTML ê²½ë¡œ")
    parser.add_argument("--no-images", action="store_true", help="í˜ì´ì§€ ì´ë¯¸ì§€ ì¶”ì¶œ ì•ˆí•¨")

    args = parser.parse_args()

    # PDF ê²½ë¡œ ê²°ì •
    if args.pdf:
        pdf_path = args.pdf
    else:
        # ê¸°ë³¸ê°’: ê´‘ëª…ì†Œì‹ 649í˜¸
        pdf_path = r"C:\Users\jmyang\Dropbox\ë°°ì •ìš°-ë”ì†Œí†µ\ë”ì†Œí†µ\ì§€ìì²´ ì†Œì‹ì§€ pdf-ì›ë³¸\ê´‘ëª…\ê´‘ëª…ì†Œì‹ 649í˜¸_ìµœì¢…ìµœì .pdf"

    if os.path.exists(pdf_path):
        output = convert_newsletter_pdf(
            pdf_path=pdf_path,
            output_path=args.output,
            city_name=args.city,
            extract_images=not args.no_images
        )
        if output:
            print(f"ë¸Œë¼ìš°ì €ì—ì„œ ì—´ê¸°: file:///{output.replace(chr(92), '/')}")
    else:
        print(f"PDF íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {pdf_path}")
        print("\nì‚¬ìš©ë²•:")
        print("  python convert_newsletter.py --pdf <PDFê²½ë¡œ> --city <ì§€ìì²´ëª…>")
        print("\nì˜ˆì‹œ:")
        print("  python convert_newsletter.py --pdf ./ì†Œì‹ì§€.pdf --city ì„œì´ˆêµ¬")
        print("  python convert_newsletter.py --pdf ./ì„±ë‚¨ì†Œì‹.pdf --city ì„±ë‚¨ì‹œ")
